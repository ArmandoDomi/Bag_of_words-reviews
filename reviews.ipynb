{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reviews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvpPb8mJl8vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "import re\n",
        "from collections import OrderedDict\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('perluniprops')\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.moses import MosesDetokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "#for google colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "def readData(filepath):\n",
        "  ''' \n",
        "  a method that gets a filepath and returns data,targets\n",
        "  '''\n",
        "  raw_data=[]\n",
        "  with open(filepath) as fp:\n",
        "    for cnt, line in enumerate(fp):\n",
        "\n",
        "        # let's read line by line \n",
        "        #print(\"Line {}: {}\".format(cnt, line))\n",
        "        raw_data.append(re.split(r'\\t', line))\n",
        "  \n",
        "  data=[]\n",
        "  targets=[]\n",
        "  for row, target in raw_data:\n",
        "\n",
        "    # now we seperate the data from the targets\n",
        "    data.append(row)\n",
        "    targets.append(int(target))\n",
        "    #print(\"row : {}  class : {}\".format(row,target))\n",
        "  \n",
        "  # return the data and the targets in dic\n",
        "\n",
        "  return {'data':data,'targets':targets}\n",
        "\n",
        "yelp=readData('/content/drive/My Drive/google_colab/reviews/yelp_labelled.txt')\n",
        "imdb=readData('/content/drive/My Drive/google_colab/reviews/imdb_labelled.txt')\n",
        "amazon=readData(\"/content/drive/My Drive/google_colab/reviews/amazon_cells_labelled.txt\")\n",
        "\n",
        "\n",
        "# we make one list from the 3 datasets\n",
        "X=yelp['data']+imdb['data']+amazon['data']\n",
        "y=yelp['targets']+imdb['targets']+amazon['targets']\n",
        "'''\n",
        "X=imdb['data']\n",
        "y=imdb['targets']\n",
        "\n",
        "X=X[0:500]\n",
        "y=y[0:500]\n",
        "'''\n",
        "data={'Sentence':X,'Class':y}\n",
        "\n",
        "# we create a dataframe\n",
        "df = pd.DataFrame(data, columns = ['Sentence', 'Class'])\n",
        "\n",
        "# we remove the duplicates \n",
        "numOfnoDup = df.drop_duplicates(keep='first',inplace=False)\n",
        "print('No of duplicate rows that are dropped:',len(df)-len(numOfnoDup))\n",
        "\n",
        "# a list with stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "porter = PorterStemmer()\n",
        "# a regexp to removing removing punctuation\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "new_sentence=[]\n",
        "all_words=[]\n",
        "\n",
        "for ind in df.index:\n",
        "  #we take each row \n",
        "  row=df['Sentence'][ind]\n",
        "  #we split the row to words and take only the words from the sentence\n",
        "  token=tokenizer.tokenize(row)\n",
        "  #also remove all stop words\n",
        "  token=[w for w in token if not w in stop_words]\n",
        "  #then we stem each word\n",
        "  token=[porter.stem(w) for w in token]\n",
        "  for w in token:\n",
        "    all_words.append(w)\n",
        "  #we create the stem sentence from the token list\n",
        "  detokens = MosesDetokenizer().detokenize(token, return_str=True)\n",
        "  new_sentence.append(detokens)\n",
        "\n",
        "\n",
        "\n",
        "#print(len(all_words))\n",
        "unique_words=list(OrderedDict.fromkeys(all_words))\n",
        "#print(len(unique_words))\n",
        "#print(\"the unique words are : {}\".format(len(unique_words.sort())))\n",
        "\n",
        "\n",
        "#we create a new colum named stem_Sentence\n",
        "df['stem_Sentence']=pd.Series(new_sentence, index=df.index)\n",
        "\n",
        "vector_Sentence=[]\n",
        "\n",
        "\n",
        "for i in df.index:\n",
        "  row=df['stem_Sentence'][i]\n",
        "  temp=[]\n",
        "  for x in unique_words:\n",
        "    #print(x)\n",
        "    num=row.count(x)\n",
        "    #print(unique_words.count(x))\n",
        "    temp.append(num)\n",
        "# we create feature vector\n",
        "  vector_Sentence.append(temp)\n",
        "\n",
        "print(len(df['Class']))\n",
        "\n",
        "'''\n",
        "for sentence in vector_Sentence:\n",
        "  print(sentence)\n",
        "'''\n",
        "\n",
        "#print(df['stem_Sentence'])\n",
        "\n",
        "#split the data to train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(vector_Sentence,df['Class'], test_size=0.3)\n",
        "'''\n",
        "# let's create a create for\n",
        "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-2,1e-3, 1e-4],\n",
        "                     'C': [1, 10, 100, 1000]},\n",
        "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
        "\n",
        "grid = GridSearchCV(svm.SVC(), tuned_parameters)\n",
        "\n",
        "grid.fit(X_train,y_train)\n",
        "\n",
        "# print best parameter after tuning \n",
        "print(grid.best_params_) \n",
        "  \n",
        "# print how our model looks after hyper-parameter tuning \n",
        "print(grid.best_estimator_)\n",
        "\n",
        "'''\n",
        "model= svm.SVC(C=1000,kernel='rbf',gamma=0.0001)\n",
        "model.fit(X_train,y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "tn, fp, fn, tp =  confusion_matrix(y_test,y_pred).ravel()\n",
        "\n",
        "print(\"tn:{} fp:{} fn:{} tp:{}\".format(tn,fp,fn,tp))\n",
        "\n",
        "accuracy=((tn+tp)/(tn+tp+fn+fp))\n",
        "precision=(tp/(tp+fp))\n",
        "recall=(tp/(tp+fn))\n",
        "fmeasure=(((precision*recall*2)/((precision+recall))))\n",
        "sensitivity=(tp/(tp+fn))\n",
        "specificity=(tn/(tn+fp))\n",
        "\n",
        "\n",
        "print(\"Test set\")\n",
        "print(\"Mean Accuracy :\",accuracy)\n",
        "print(\"Mean Precision :\",precision)\n",
        "print(\"Mean recall :\",recall)   \n",
        "print(\"Mean Fmeasure :\",fmeasure)\n",
        "print(\"Mean Sensitivity :\",sensitivity)\n",
        "print(\"Mean Specificity :\",specificity)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}